# AGENTS — Atticus

> **Purpose**: Atticus is a Retrieval‑Augmented Generation (RAG) assistant that helps **Sales** answer faster, reduces **Service/Marketing** interruptions, and accelerates **tender** responses. It ingests curated content, retrieves the most relevant passages, and generates grounded answers with inline citations. When confidence is low, it provides a careful partial answer and escalates via email.
> **Audience**: Internal (Sales, Marketing, National Technical Specialists, Engineering).
> **Docs Style**: Detailed, explicit, and operationally useful.

---

## 1) System Overview

### 1.1 Components & Responsibilities

| Stage                       | What Happens                                                                                         | Key Components                   |
| --------------------------- | ---------------------------------------------------------------------------------------------------- | -------------------------------- |
| **Ingest & Index**          | Parse → chunk → embed → persist index                                                                | Python scripts, FAISS, BM25‑lite |
| **Retrieve & Rank**         | Hybrid dense + keyword retrieval (re‑ranker: planned)                                                | FAISS, BM25‑lite                 |
| **Generate Answer**         | `GEN_MODEL` drafts a concise, sourced reply                                                          | GPT‑4.1                          |
| **Confidence & Escalation** | If `confidence < CONFIDENCE_THRESHOLD`, return a careful partial answer and send an escalation email | SES SMTP                         |
| **UI Contract**             | Web UI at `/` with left‑menu **CONTACT** and right‑side chat                                         | FastAPI + Jinja2                 |

### 1.2 Lifecycle (Order of Operations)

1. **Add/Update Content** under `content/` using `YYYYMMDD_topic_version.ext` naming.
2. **Ingest** with `make ingest` → parse, chunk, embed, rebuild index.
3. **Evaluate** retrieval with `make eval` → metrics in `eval/runs/<timestamp>/metrics.json`.
4. **Serve** with `make api` → UI + `/ask` endpoint.
5. **Escalate** automatically when confidence falls below threshold.
6. **Observe** via `logs/app.jsonl` and `logs/errors.jsonl` (or `/admin/sessions`).
7. **Release**: commit updated `indices/manifest.json` and tag.

---

## 2) Configuration

Primary settings come from `.env` generated by `scripts/generate_env.py`. Host env vars can override unless `ATTICUS_ENV_PRIORITY=env` is set.

| Category             | Keys (examples)                                                                  | Defaults / Notes                                              |
| -------------------- | -------------------------------------------------------------------------------- | ------------------------------------------------------------- |
| **Models**           | `GEN_MODEL`, `EMBED_MODEL`, `EMBEDDING_MODEL_VERSION`                            | `gpt-4.1`, `text-embedding-3-large@2025-01-15`                |
| **Retrieval**        | `CONFIDENCE_THRESHOLD`, `MAX_CONTEXT_CHUNKS`                                     | **Default `0.70`** (configured in `.env`; changeable per‑env) |
| **Chunking**         | `CHUNK_TARGET_TOKENS`, `CHUNK_MIN_TOKENS`, `CHUNK_OVERLAP_TOKENS`                | Balance context/speed                                         |
| **Content**          | `CONTENT_DIR`                                                                    | Default `./content`                                           |
| **Email (SES SMTP)** | `CONTACT_EMAIL`, `SMTP_HOST`, `SMTP_PORT`, `SMTP_USER`, `SMTP_PASS`, `SMTP_FROM` | Use SES **SMTP credentials**, not IAM keys                    |

### 2.1 `.env.example`

Copy to `.env` and fill in secrets (leave keys blank for local).

```dotenv
ATTICUS_ENV_PRIORITY=env
OPENAI_MODEL=gpt-4.1
OPENAI_API_KEY=
EMBED_MODEL=text-embedding-3-large
EMBEDDING_MODEL_VERSION=text-embedding-3-large@2025-01-15
GEN_MODEL=gpt-4.1
CONFIDENCE_THRESHOLD=0.70
CHUNK_TARGET_TOKENS=512
CHUNK_MIN_TOKENS=256
CHUNK_OVERLAP_TOKENS=100
MAX_CONTEXT_CHUNKS=10
LOG_LEVEL=INFO
LOG_VERBOSE=1
LOG_TRACE=1
TIMEZONE=Australia/Sydney
EVAL_REGRESSION_THRESHOLD=3.0
CONTENT_DIR=./content
CONTACT_EMAIL=
# Team routing for escalations (used by Email JSON routing)
TEAM_EMAIL_NTS=
TEAM_EMAIL_MARKETING=
TEAM_EMAIL_SERVICE=
ADMIN_EMAIL=
# Routing keyword terms (comma-separated; editable by Sales/Marketing without code)
ROUTE_TERMS_TECHNICAL=spec,dpi,driver,firmware,toner,icc,calibration
ROUTE_TERMS_MARKETING=brochure,pricing,campaign,roi,feature,benefit
ROUTE_TERMS_SERVICE=service,maintenance,warranty,engineer,onsite
# Escalation logging locations
ESCALATION_LOG_JSON=logs/escalations.jsonl
ESCALATION_LOG_CSV=logs/escalations.csv
ESCALATION_COUNTER_FILE=logs/escalations.counter
# SES SMTP (production uses SMTP credentials, not IAM)
SMTP_HOST=email-smtp.ap-southeast-2.amazonaws.com
SMTP_PORT=587
SMTP_USER=
SMTP_PASS=
SMTP_FROM=
SMTP_TO=
# UI behaviour toggles
UI_SHOW_ESCALATION_BANNER=1
UI_BANNER_DISMISS_TIMEOUT_MS=12000
```

dotenv
ATTICUS_ENV_PRIORITY=env
OPENAI_MODEL=gpt-4.1
OPENAI_API_KEY=
EMBED_MODEL=text-embedding-3-large
EMBEDDING_MODEL_VERSION=text-embedding-3-large@2025-01-15
GEN_MODEL=gpt-4.1
CONFIDENCE_THRESHOLD=0.70
CHUNK_TARGET_TOKENS=512
CHUNK_MIN_TOKENS=256
CHUNK_OVERLAP_TOKENS=100
MAX_CONTEXT_CHUNKS=10
LOG_LEVEL=INFO
LOG_VERBOSE=1
LOG_TRACE=1
TIMEZONE=Australia/Sydney
EVAL_REGRESSION_THRESHOLD=3.0
CONTENT_DIR=./content
CONTACT_EMAIL=

#### Team routing for escalations (used by Email JSON routing)

TEAM_EMAIL_NTS=
TEAM_EMAIL_MARKETING=
TEAM_EMAIL_SERVICE=
ADMIN_EMAIL=

#### Escalation logging locations

ESCALATION_LOG_JSON=logs/escalations.jsonl
ESCALATION_LOG_CSV=logs/escalations.csv
ESCALATION_COUNTER_FILE=logs/escalations.counter

#### SES SMTP (production uses SMTP credentials, not IAM)

SMTP_HOST=email-smtp.ap-southeast-2.amazonaws.com
SMTP_PORT=587
SMTP_USER=
SMTP_PASS=
SMTP_FROM=
SMTP_TO=

```dotenv
ATTICUS_ENV_PRIORITY=env
OPENAI_MODEL=gpt-4.1
OPENAI_API_KEY=
EMBED_MODEL=text-embedding-3-large
EMBEDDING_MODEL_VERSION=text-embedding-3-large@2025-01-15
GEN_MODEL=gpt-4.1
CONFIDENCE_THRESHOLD=0.70
CHUNK_TARGET_TOKENS=512
CHUNK_MIN_TOKENS=256
CHUNK_OVERLAP_TOKENS=100
MAX_CONTEXT_CHUNKS=10
LOG_LEVEL=INFO
LOG_VERBOSE=1
LOG_TRACE=1
TIMEZONE=Australia/Sydney
EVAL_REGRESSION_THRESHOLD=3.0
CONTENT_DIR=./content
CONTACT_EMAIL=
SMTP_HOST=email-smtp.ap-southeast-2.amazonaws.com
SMTP_PORT=587
SMTP_USER=
SMTP_PASS=
SMTP_FROM=
SMTP_TO=
```

Diagnostics:

```bash
python scripts/debug_env.py
```

---|---|---|
| **Models** | `GEN_MODEL`, `EMBED_MODEL`, `EMBEDDING_MODEL_VERSION` | `gpt-4.1`, `text-embedding-3-large@2025-01-15` |
| **Retrieval** | `CONFIDENCE_THRESHOLD`, `MAX_CONTEXT_CHUNKS` | **Default `0.70`** (configured in `.env`; changeable per‑env) |
| **Chunking** | `CHUNK_TARGET_TOKENS`, `CHUNK_MIN_TOKENS`, `CHUNK_OVERLAP_TOKENS` | Balance context/speed |
| **Content** | `CONTENT_DIR` | Default `./content` |
| **Email (SES SMTP)** | `CONTACT_EMAIL`, `SMTP_HOST`, `SMTP_PORT`, `SMTP_USER`, `SMTP_PASS`, `SMTP_FROM` | Use SES **SMTP credentials**, not IAM keys |

Diagnostics:

```bash
python scripts/debug_env.py
```

---

## 3) Architecture Documentation Standards

* **Documentation model**: Use **Diátaxis** — Tutorials, How‑to Guides, Reference, Explanation — to keep audience and intent clear. Place content accordingly under `/docs/`.
* **Diagrams**: Use the **C4 model** (Context → Container → Component; add Code if required). Render in Markdown with Mermaid for portability.
* **Decisions**: Record non‑trivial choices as **ADRs** (Architecture Decision Records). One ADR per decision: Context → Decision → Consequences.

> Minimal folder seed:
>
> ```bash
> docs/
>   tutorials/
>   how-to/
>   reference/
>   explanation/
> adr/
>   0001-record-architecture-decisions.md
> ```

### 3.1 Service Layer Pattern

* **Pattern**: *API Route → Service → Database*.
* **Routes** remain thin (validation, auth, shape responses); **Services** contain orchestration and domain logic; **Data** layer isolates persistence concerns.
* **Error handling** in services should use explicit exception types surfaced by FastAPI handlers.

### 3.2 Error-Handling Patterns (Atticus)

* **Fail fast** on startup/config/auth/DB errors; surface clear messages and stop the process.
* **Skip & continue** in batch/ingest flows; never persist corrupted/partial items.
* **Detail-rich errors**: include operation context, IDs, and retain stack traces in logs (`exc_info=True`).
* **Batch reporting**: always return counts and per-item failure details for ingest/eval.

---

## 4) Repository Structure & Conventions

Use a **`src/` layout** for import correctness and packaging hygiene.

```bash
repo-root/
├─ .github/
│  ├─ workflows/ci.yml
│  ├─ ISSUE_TEMPLATE.md
│  └─ PULL_REQUEST_TEMPLATE.md
├─ docs/                 # Diátaxis structure
│  ├─ tutorials/
│  ├─ how-to/
│  ├─ reference/
│  └─ explanation/
├─ adr/
│  └─ 0001-record-architecture-decisions.md
├─ src/
│  └─ atticus/          # application package
├─ tests/
├─ scripts/
├─ infra/               # Docker/compose/k8s/IaC
├─ content/
├─ indices/
├─ eval/
├─ logs/
├─ .editorconfig
├─ .pre-commit-config.yaml
├─ pyproject.toml
├─ Makefile
├─ README.md
├─ CHANGELOG.md
├─ CONTRIBUTING.md
├─ CODE_OF_CONDUCT.md
├─ CODEOWNERS
├─ requirements.in      # declared (unpinned)
└─ requirements.txt     # compiled (pinned)
```

**Ownership & reviews**: Use `CODEOWNERS` + branch protection (require PR reviews and green CI) so quality gates are enforced.

---

## 5) Quality Gates (Makefile‑first)

All tests and checks run in a **logical order** and are exposed via `make` targets. CI blocks merges unless these pass.

* **Format & Lint**: Ruff (formatter + linter + isort rules)
* **Types**: mypy (start strict; add targeted ignores for third‑party libs)
* **Tests**: pytest with coverage; fail under ≥90% (critical modules can have per‑file thresholds)
* **Evaluation**: retrieval metrics (fail CI on regression beyond allowed delta)

Suggested Make targets (aligned with README):

| Target         | What it does                                            |
| -------------- | ------------------------------------------------------- |
| `make fmt`     | Format code (Ruff formatter) + auto‑fix lint where safe |
| `make lint`    | Lint code (Ruff)                                        |
| `make type`    | Static typing (mypy)                                    |
| `make test`    | Run unit/integration tests                              |
| `make cov`     | Coverage HTML + console report                          |
| `make quality` | fmt → lint → type → test → cov                          |
| `make ingest`  | Parse, chunk, embed, update index                       |
| `make eval`    | Retrieval evaluation, write metrics                     |
| `make api`     | Run FastAPI + UI                                        |
| `make e2e`     | Ingest → Eval → API smoke → UI ping                     |

---

## 6) Error Handling & API Contract

Return **structured JSON** with a unique `request_id`. Keep stack traces in server logs; never log secrets. Skip corrupted data rather than persisting it.

### **Confidence policy**

* `CONFIDENCE_THRESHOLD` **= 0.70 (default)**, set in `.env`.
* If model confidence `< 0.70`: return a **careful partial answer** with explicit uncertainty + sources retrieved; trigger **email escalation**.
* If confidence `≥ 0.70`: return full answer with citations.

Examples:

```json
200 OK
{
  "answer": "…",
  "sources": [{"path": "content/example.pdf", "page": 3}],
  "confidence": 0.82,
  "request_id": "abc123"
}
```

```json
206 Partial (low confidence)
{
  "answer": "This may be incomplete. Based on available sources, ...",
  "sources": [{"path": "content/guide.pdf", "page": 7}],
  "confidence": 0.61,
  "escalated": true,
  "request_id": "abc123"
}
```

```json
400 Validation
{
  "error": "validation_error",
  "detail": "'query' is required",
  "fields": {"query": "missing"},
  "request_id": "abc123"
}
```

```json
422 Partial ingestion
{
  "status": "partial",
  "succeeded": 12,
  "failed": 1,
  "errors": [{"doc": "/content/x.pdf", "reason": "pdf parse failed"}],
  "request_id": "abc123"
}
```

```json
5xx Internal
{
  "error": "internal_error",
  "detail": "see logs",
  "request_id": "abc123"
}
```

### 6.1 Escalation & Output Schemas (Atticus‑only)

This section codifies **exact structures** Atticus must emit.

#### 6.1.1 Email Escalation JSON (rendered to email)

**Routing logic** (Atticus decides):

* *Technical question* → **NTS Team**
* *Marketing question* → **Marketing Team**
* *Service question* → **Service Team**
* *Unsure* → **NTS Team**

**Subject format**: `Escalation from Atticus: AE<INT>` with `<INT>` **starting at 100**, no zero‑padding. Examples: `AE100`, `AE101`, … `AE999`, `AE1000`, etc. The `<INT>` must **increment** and be **recorded** in both a spreadsheet (CSV) and a JSON log.

```json
{
  "to": ["team@example.com"],
  "cc": ["<user_email>", "<admin_email>"],
  "subject": "Escalation from Atticus: AE00432",
  "body": {
    "header": "Escalation from Atticus: AE00432.",
    "question": "<user question>",
    "answer": "<current best partial/complete answer>",
    "certainty_score": 0.61,
    "certainty_reason": "<why confidence is low>",
    "citations": [
      {"path": "content/x.pdf", "page": 3},
      {"path": "content/y.md", "anchor": "#specs"}
    ]
  },
  "metadata": {
    "ae_id": "AE00432",
    "request_id": "abc123",
    "timestamp": "2025-09-26T12:34:56+10:00",
    "category": "technical|marketing|service|unsure"
  }
}
```

#### **Logging requirements**

* Append one line per escalation to `logs/escalations.jsonl` (the full JSON above).
* Append a row to `logs/escalations.csv` with columns:
  `ae_id, timestamp, category, certainty_score, question, answer_preview, to, cc, request_id`.
* Persist **counter** for `AE` IDs at `logs/escalations.counter`.

  * If the file is **absent**, initialise it to `100` (so first ID is `AE100`).
  * On each escalation: read → increment → write atomically.
  * The counter **continues past 999** (e.g., `AE1000`, `AE1001`, …).

#### 6.1.2 Answer JSON (LLM reply contract)

```json
{
  "answer": "One to two sentences, crisp summary.",
  "bullets": ["Optional", "Only if it improves readability"],
  "citations": [
    {"path": "content/a.pdf", "page": 2},
    {"path": "content/b.html", "selector": "#table-1"}
  ]
}
```

Rules:

* Keep `answer` to **1–2 sentences** when possible.
* Include `bullets` **only if** it makes the summary cleaner.
* Up to **3 citations**; prefer the **most authoritative** sources.

#### 6.1.3 Chat JSON (first message on page load)

```bash
    Hello, I'm Atticus.
    I'm here to answer all your product related questions.
    - If it is related to a specific model, let me know when you're asking a question.
    - If you need help deciding which model is the best fit, tell me your requirements.

    Anything that I can't answer will be raised to the right team for their expert assistance.

    Whenever you're ready, let's begin.
```

#### **Clarification policy**

* If the user’s question is **ambiguous or missing key fields**, Atticus must ask **one targeted clarification question** before answering.
* If ambiguity remains after one clarification, answer conservatively and include uncertainty notes; escalate if confidence `< 0.70`.

---

## 7) Logging & Observability

* **Default**: human‑readable console logging with levels.
* **Verbose/JSON mode**: toggle via env (e.g., `LOG_VERBOSE=1`) to emit structured JSON (include request/correlation IDs).
* **Tracing**: integrate **OpenTelemetry** to inject trace/span IDs into logs and export OTLP to the chosen backend.
* **Log files**: `logs/app.jsonl` (info) and `logs/errors.jsonl` (errors).

---

## 8) Security Guardrails

* `.env` is required; **no hard‑coded secrets**.
* Use **SES SMTP credentials** (not IAM keys).
* Restrict `ses:FromAddress` to approved senders/region; SES sandbox rules apply where relevant.
* Redact PII in logs and traces.

---

## 9) Evaluation (Retrieval Quality)

* Maintain a gold set under `eval/`.
* `make eval` calculates retrieval metrics; CI fails on regression beyond the configured tolerance.
* **Confidence-binned reporting**: track metrics in bins `>=0.80`, `0.70–0.79`, `0.60–0.69`, `<0.60`.

  * CI rule: **no bin crossing the threshold (0.70)** may regress by more than `${EVAL_REGRESSION_THRESHOLD}%` versus baseline.
  * Emit a table and JSON like `eval/runs/<ts>/metrics.json` with per-bin precision/recall and hit-rate@k.
* Tie acceptance tests to **confidence behavior**:

  * At `CONFIDENCE_THRESHOLD = 0.70`, require: (a) **Precision@k** does not regress > X%, (b) **Recall@k** ≥ baseline, (c) **Low‑confidence path** returns `206 Partial` and sets `escalated=true`.
* Store run artefacts under `eval/runs/<timestamp>/` with `metrics.json` for traceability.

---

## 10) File/Folder Glossary

| Location   | Purpose                                        |
| ---------- | ---------------------------------------------- |
| `content/` | Source documents to ingest                     |
| `indices/` | Vector index + `manifest.json` snapshot        |
| `eval/`    | Gold sets + evaluation runs                    |
| `logs/`    | `app.jsonl` (info) and `errors.jsonl` (errors) |

---

## 11) Release & Change Management

**Policy**: We adopt **Semantic Versioning** and **Conventional Commits** end‑to‑end. CI auto‑generates the changelog and bumps versions on release branches/tags.

### 11.1 Versioning

* **SemVer**: `MAJOR.MINOR.PATCH`.
* `MAJOR` for breaking API/contract changes, `MINOR` for new features (backward compatible), `PATCH` for fixes.

### 11.2 Conventional Commits (required)

* Format: `<type>(<scope>)!: <short summary>`
* Body: free‑form details, bullet points allowed.
* Footer: `BREAKING CHANGE:` … (if not using the `!` bang) and issue refs like `Fixes #123`.

**Allowed types**: `feat`, `fix`, `docs`, `style`, `refactor`, `perf`, `test`, `build`, `ci`, `chore`, `revert`.

### **Examples**

* `feat(retrieval): add hybrid BM25+dense with top_k=8`
* `fix(ui): handle empty query gracefully`
* `refactor(ingest)!: switch chunker to token‑aware splitter`
* `docs(adr): record decision for SES SMTP over IAM`

### 11.3 Tooling

* **Commit check (local)**: `pre-commit` runs Conventional Commit **lint** via Commitizen.
* **Changelog + version bump**: Commitizen writes `CHANGELOG.md` and updates the version in `pyproject.toml`.

> Add this to `.pre-commit-config.yaml`:
>
> ```yaml
> - repo: https://github.com/commitizen-tools/commitizen
>   rev: v3.30.1
>   hooks:
>     - id: commitizen
>       stages: [commit-msg]
> ```
>
> Configure Commitizen in `pyproject.toml`:
>
> ```toml
> [tool.commitizen]
> name = "cz_conventional_commits"
> version = "0.1.0"
> tag_format = "v$version"
> update_changelog_on_bump = true
> changelog_file = "CHANGELOG.md"
> ```

### **Makefile additions**

* `make release`: runs `cz bump` (interactive) → updates version + `CHANGELOG.md` → creates tag.

```make
release:
	cz bump --yes
	git push --follow-tags
```

### 11.4 CI release job (GitHub Actions)

Creates GitHub Release from the tagged version and attaches artefacts.

```yaml
# .github/workflows/release.yml
name: Release
on:
  push:
    tags: [ "v*" ]
jobs:
  release:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with: { python-version: '3.12' }
      - run: pip install commitizen
      - name: Generate changelog (idempotent)
        run: cz changelog --dry-run > /dev/null || true
      - name: Create GitHub Release
        uses: softprops/action-gh-release@v2
        with:
          generate_release_notes: true
```

### **Branch protection**

* Require green CI and Code Owner reviews on `main`.
* Require Conventional Commit titles (enforced by commit‑msg hook above).

---

## 12) Open Questions / TODOs

* Define a precise **escalation vs. refusal** policy (content categories, thresholds, user messaging).
* Re‑ranker deployment strategy and metrics (when added).

---

## 13) Operational Helpers (Scripts & Make targets)

### 13.1 `scripts/next_ae_id.py`

Generates the next AE ID from the counter file, initialising to **100** if absent. Safe across platforms using a temp file + atomic replace.

```python
#!/usr/bin/env python3
from __future__ import annotations
import argparse, os, sys, tempfile
from pathlib import Path

DEFAULT_COUNTER = Path(os.getenv("ESCALATION_COUNTER_FILE", "logs/escalations.counter"))


def read_counter(path: Path) -> int:
    try:
        with path.open("r", encoding="utf-8") as f:
            return int(f.read().strip())
    except FileNotFoundError:
        return 100  # first AE is AE100
    except Exception as e:
        print(f"error: failed to read counter {path}: {e}", file=sys.stderr)
        return 100


def write_counter(path: Path, value: int) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    # atomic write: write to temp then replace
    with tempfile.NamedTemporaryFile("w", delete=False, dir=str(path.parent), encoding="utf-8") as tmp:
        tmp.write(str(value))
        tmp_path = Path(tmp.name)
    os.replace(tmp_path, path)


def next_ae(path: Path) -> str:
    current = read_counter(path)
    ae_id = f"AE{current}"
    write_counter(path, current + 1)
    return ae_id


def main() -> int:
    p = argparse.ArgumentParser(description="Emit next AE ID and advance the counter.")
    p.add_argument("--counter", type=Path, default=DEFAULT_COUNTER, help="Path to AE counter file")
    args = p.parse_args()

    try:
        print(next_ae(args.counter))
        return 0
    except Exception as e:
        print(f"error: {e}", file=sys.stderr)
        return 1


if __name__ == "__main__":
    raise SystemExit(main())
```

### **Usage**

```bash
python scripts/next_ae_id.py                 # -> AE100 (and increments)
python scripts/next_ae_id.py --counter logs/escalations.counter
```

### 13.2 `scripts/log_escalation.py`

Append the escalation to JSONL and CSV.

```python
#!/usr/bin/env python3
from __future__ import annotations
import argparse, csv, json, os, sys
from dataclasses import asdict, dataclass
from datetime import datetime, timezone
from pathlib import Path

JSON_LOG = Path(os.getenv("ESCALATION_LOG_JSON", "logs/escalations.jsonl"))
CSV_LOG = Path(os.getenv("ESCALATION_LOG_CSV", "logs/escalations.csv"))
TZ = os.getenv("TIMEZONE", "Australia/Sydney")

@dataclass
class Escalation:
    ae_id: str
    category: str
    certainty_score: float
    question: str
    answer_preview: str
    to: str
    cc: str
    request_id: str
    timestamp: str | None = None

    def normalise(self) -> None:
        if not self.timestamp:
            # naive ISO in local; servers may prefer UTC with offset
            self.timestamp = datetime.now(timezone.utc).isoformat()


def append_jsonl(path: Path, esc: Escalation) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("a", encoding="utf-8") as f:
        f.write(json.dumps(asdict(esc), ensure_ascii=False) + "
")


def append_csv(path: Path, esc: Escalation) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    header = [
        "ae_id", "timestamp", "category", "certainty_score", "question",
        "answer_preview", "to", "cc", "request_id",
    ]
    exists = path.exists()
    with path.open("a", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        if not exists:
            w.writerow(header)
        w.writerow([
            esc.ae_id, esc.timestamp, esc.category, esc.certainty_score,
            esc.question, esc.answer_preview, esc.to, esc.cc, esc.request_id,
        ])


def main() -> int:
    ap = argparse.ArgumentParser(description="Append escalation to JSONL and CSV logs")
    ap.add_argument("ae_id")
    ap.add_argument("category", choices=["technical", "marketing", "service", "unsure"])
    ap.add_argument("certainty_score", type=float)
    ap.add_argument("question")
    ap.add_argument("answer_preview")
    ap.add_argument("to")
    ap.add_argument("cc")
    ap.add_argument("request_id")
    ap.add_argument("--json", default=str(JSON_LOG))
    ap.add_argument("--csv", default=str(CSV_LOG))
    args = ap.parse_args()

    esc = Escalation(
        ae_id=args.ae_id,
        category=args.category,
        certainty_score=args.certainty_score,
        question=args.question,
        answer_preview=args.answer_preview,
        to=args.to,
        cc=args.cc,
        request_id=args.request_id,
    )
    esc.normalise()

    append_jsonl(Path(args.json), esc)
    append_csv(Path(args.csv), esc)
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
```

### **Usage**

```bash
AE=$(python scripts/next_ae_id.py)
python scripts/log_escalation.py "$AE" technical 0.61 "What is X?" "Partial answer…" nts@example.com "user@example.com,admin@example.com" req-123
```

### 13.3 Make targets

```make
.PHONY: next-ae log-escalation

next-ae:
	@python scripts/next_ae_id.py

# Example: make log-escalation AE=AE123 CAT=technical SCORE=0.61 Q="What is X?" A="Partial…" TO=nts@example.com CC="user@example.com,admin@example.com" RID=req-123
log-escalation:
	@python scripts/log_escalation.py "$(AE)" "$(CAT)" "$(SCORE)" "$(Q)" "$(A)" "$(TO)" "$(CC)" "$(RID)"
```

---

### 13.4 API Integration Sketch (FastAPI)

Low‑confidence flow: compute AE → log → build Email JSON payload → send (stub) → return 206.

```python
# api/routes/ask.py (sketch)
from __future__ import annotations
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
import os
from pathlib import Path

from scripts.next_ae_id import next_ae
from scripts.log_escalation import append_jsonl, append_csv, Escalation  # if refactored into importable funcs
from scripts.send_email import send_email  # new stub below

router = APIRouter()

TEAM_EMAILS = {
    "technical": os.getenv("TEAM_EMAIL_NTS", "nts@example.com"),
    "marketing": os.getenv("TEAM_EMAIL_MARKETING", "marketing@example.com"),
    "service": os.getenv("TEAM_EMAIL_SERVICE", "service@example.com"),
}
ADMIN_EMAIL = os.getenv("ADMIN_EMAIL", "admin@example.com")

# Parse routing terms from env
TERMS = {
    "technical": set(map(str.strip, os.getenv("ROUTE_TERMS_TECHNICAL", "").split(","))) - {""},
    "marketing": set(map(str.strip, os.getenv("ROUTE_TERMS_MARKETING", "").split(","))) - {""},
    "service": set(map(str.strip, os.getenv("ROUTE_TERMS_SERVICE", "").split(","))) - {""},
}

class AskRequest(BaseModel):
    query: str

class Citation(BaseModel):
    path: str
    page: int | None = None
    anchor: str | None = None

class AnswerJSON(BaseModel):
    answer: str
    bullets: list[str] | None = None
    citations: list[Citation] = []

# Explicit API response contracts
class AnswerResponse(BaseModel):
    answer: str
    sources: list[Citation] = []
    confidence: float
    request_id: str

class PartialResponse(AnswerResponse):
    escalated: bool = True
    ae_id: str | None = None

CONFIDENCE_THRESHOLD = float(os.getenv("CONFIDENCE_THRESHOLD", 0.70))


def route_category(text: str) -> str:
    t = (text or "").lower()
    for cat, terms in TERMS.items():
        if any(term in t for term in terms):
            return cat
    return "unsure"


@router.post("/ask", response_model=AnswerResponse)
async def ask(req: AskRequest):
    # 1) Retrieve + generate (pseudo)
    answer_json = AnswerJSON(answer="<generated answer>")
    confidence = 0.61  # example
    request_id = "req-123"  # inject real ID

    if confidence < CONFIDENCE_THRESHOLD:
        # 2) Build escalation
        ae_id = next_ae(Path(os.getenv("ESCALATION_COUNTER_FILE", "logs/escalations.counter")))
        category = route_category(answer_json.answer)
        to = TEAM_EMAILS.get(category, TEAM_EMAILS["technical"])
        cc = ",".join(filter(None, [os.getenv('CONTACT_EMAIL'), ADMIN_EMAIL]))

        email_payload = {
            "to": [to],
            "cc": [e.strip() for e in cc.split(",") if e.strip()],
            "subject": f"Escalation from Atticus: {ae_id}",
            "body": {
                "header": f"Escalation from Atticus: {ae_id}.",
                "question": req.query,
                "answer": answer_json.answer,
                "certainty_score": confidence,
                "certainty_reason": "Below threshold",
                "citations": [c.model_dump() for c in answer_json.citations][:3],
            },
            "metadata": {
                "ae_id": ae_id,
                "request_id": request_id,
                "category": category,
            },
        }

        # 3) Log
        esc = Escalation(
            ae_id=ae_id,
            category=category,
            certainty_score=confidence,
            question=req.query,
            answer_preview=answer_json.answer[:160],
            to=to,
            cc=cc,
            request_id=request_id,
        )
        append_jsonl(Path(os.getenv("ESCALATION_LOG_JSON", "logs/escalations.jsonl")), esc)
        append_csv(Path(os.getenv("ESCALATION_LOG_CSV", "logs/escalations.csv")), esc)

        # 4) Send escalation email (retry/backoff handled in send_email stub)
        send_email(email_payload)

        # 5) Return Partial with 206 and optional banner flag in UI settings
        return PartialResponse(
            answer="This may be incomplete. I've escalated it for expert review.",
            sources=answer_json.citations,
            confidence=confidence,
            request_id=request_id,
            escalated=True,
            ae_id=ae_id,
        )

    # Success path
    return AnswerResponse(
        answer=answer_json.answer,
        sources=answer_json.citations,
        confidence=confidence,
        request_id=request_id,
    )
```

### **UI banner**

* If `UI_SHOW_ESCALATION_BANNER=1` and a `206 Partial` is returned, show: `Escalated as {ae_id}. You’ll receive an email when there’s an update.`
* Dismiss automatically after `UI_BANNER_DISMISS_TIMEOUT_MS`.

---

### 13.5 `scripts/send_email.py`

SES SMTP sender with retry/backoff and a dry‑run mode.

```python
#!/usr/bin/env python3
from __future__ import annotations
import os, smtplib, ssl, time, json
from email.message import EmailMessage

HOST = os.getenv("SMTP_HOST")
PORT = int(os.getenv("SMTP_PORT", "587"))
USER = os.getenv("SMTP_USER")
PASS = os.getenv("SMTP_PASS")
FROM = os.getenv("SMTP_FROM")
DRY_RUN = os.getenv("EMAIL_DRY_RUN", "0") == "1"


def build_message(payload: dict) -> EmailMessage:
    msg = EmailMessage()
    msg["From"] = FROM
    msg["To"] = ", ".join(payload["to"]) if isinstance(payload["to"], list) else payload["to"]
    if payload.get("cc"):
        msg["Cc"] = ", ".join(payload["cc"]) if isinstance(payload["cc"], list) else payload["cc"]
    msg["Subject"] = payload["subject"]

    body = payload.get("body", {})
    lines = [
        body.get("header", "Escalation from Atticus."),
        "",
        f"Question: {body.get('question','')}",
        f"Answer: {body.get('answer','')}",
        f"Certainty Score: {body.get('certainty_score','')}",
        f"Reason: {body.get('certainty_reason','')}",
    ]
    cites = body.get("citations", [])
    if cites:
        lines.append("")
        for i, c in enumerate(cites[:3], start=1):
            lines.append(f"Citation {i}: {json.dumps(c, ensure_ascii=False)}")

    msg.set_content("
".join(lines))
    return msg


def send_email(payload: dict, retries: int = 3, backoff: float = 1.5) -> None:
    if DRY_RUN:
        print("[DRY-RUN] Would send email:", json.dumps(payload, ensure_ascii=False))
        return

    msg = build_message(payload)
    context = ssl.create_default_context()

    for attempt in range(1, retries + 1):
        try:
            with smtplib.SMTP(HOST, PORT) as server:
                server.starttls(context=context)
                server.login(USER, PASS)
                server.send_message(msg)
            return
        except Exception as e:
            if attempt == retries:
                raise
            time.sleep(backoff ** attempt)
```

### **Make additions**

```make
.PHONY: send-email
send-email:
	@python scripts/send_email.py
```

---

### 13.6 Pydantic models (documentation)

FastAPI will auto‑document `AnswerResponse` and `PartialResponse` at `/docs` so the UI and API can’t drift apart.

---

### 13.7 Pre-commit secrets scanning (Gitleaks)

Add a baseline scanner to prevent secrets from entering the repo, with an allowlist to reduce noise.

**`.pre-commit-config.yaml` addition**

```yaml
- repo: https://github.com/gitleaks/gitleaks
  rev: v8.18.4
  hooks:
    - id: gitleaks
      args: ["protect", "--verbose", "--redact", "--staged", "--config=.gitleaks.toml"]
```

**`.gitleaks.toml` (minimal)**

```toml
[extend]
# Add custom false-positive rules here

[allowlist]
# Example: allow local test keys or documented placeholders
regexes = [
  "^OPENAI_API_KEY=$",
  "^SMTP_PASS=$",
]
paths = [
  "docs/",
]
```

---

### 13.8 Telemetry correlation (request_id everywhere)

* Generate a **`request_id`** for every `/ask` call.
* Include it in:

  * JSON response (`request_id`)
  * logs/traces (logger MDC / OpenTelemetry attributes)
  * **email subject** as `· {request_id}`: e.g., `Escalation from Atticus: AE123 · req-789`
* Benefit: copy‑paste any one of AE ID / request ID into search to pull the whole story.

### **API sketch adjustment**

```python
request_id = make_request_id()  # e.g., ulid/uuid-v7
email_payload["subject"] += f" · {request_id}"
```

---

### 13.9 CI Smoke tests (ingest + success + low‑confidence)

Guarantees both happy‑path and 206‑Partial path stay healthy.

**`tests/test_smoke.py` (sketch)**

```python
from __future__ import annotations
import os
from fastapi.testclient import TestClient
from api.main import app

def test_success_and_partial_paths(tmp_path, monkeypatch):
    # Force threshold and a deterministic partial
    monkeypatch.setenv("CONFIDENCE_THRESHOLD", "0.70")
    client = TestClient(app)

    # Success path (fake confidence >= 0.70)
    r1 = client.post("/ask", json={"query": "What is the print resolution?"})
    assert r1.status_code == 200
    j1 = r1.json()
    assert "answer" in j1 and "request_id" in j1

    # Partial path (force confidence < 0.70)
    monkeypatch.setenv("FORCE_LOW_CONFIDENCE", "1")  # implemented in generator layer for tests
    r2 = client.post("/ask", json={"query": "Ambiguous question"})
    assert r2.status_code == 206
    j2 = r2.json()
    assert j2["escalated"] is True and j2["ae_id"].startswith("AE")
```

### **Makefile**

```make
.PHONY: smoke
smoke:
	pytest -q tests/test_smoke.py
```

**CI**
Add a step after `make quality`:

```yaml
- run: make smoke
```

## 14) Prompts & Agents

### 14.1 RAG System Prompt (baseline)

* Capabilities: semantic search, source filtering, synthesize multi-source answers, **always cite**.
* Approach: understand → search → analyze → synthesize → cite.
* Response: direct answer with quotes and citations; admit when unknown; suggest follow‑ups.

### 14.2 Document Agent Prompt (baseline)

* Capabilities: create/update PRDs, specs, API docs, meeting notes; maintain version history.
* Guardrails: confirm before destructive edits; log changes.

> **Implementation note**: Store named prompts in a datastore (e.g., Supabase table `archon_prompts`) with hot‑reload support at runtime. Provide `PromptService` to fetch by name and cache.

### 14.3 Agent Execution Rules

* **Task-first**: agents consult TODO/tasks and research (short keyword queries) before coding.
* **Autonomous within a task**: complete the full requested work; do not pause mid‑stream for permission if already specified.
* **Versioning**: commit/push when Definition‑of‑Done and Conventional Commits criteria are met.

---

## 15) Embedding & Indexing Standards

* **Source policy (CED PDFs & rich docs)**: preserve tables, diagrams, and images (OCR where needed); retain **breadcrumbs/metadata**.
* **Chunking**: by paragraph with **20–30% overlap**; token‑aware splitter preferred.
* **Metadata**: include source path, title, section, page, headings, and ingestion timestamp.
* **Accuracy over speed**: tune for niche technical questions; test with gold sets.

---

## 16) Data Store & Backups

* Choose a vector‑capable store suited to scale (e.g., PostgreSQL + `pgvector` / Supabase) for embeddings; ensure nightly backups for production.
* Maintain schemas for sources, chunks, prompts, projects/tasks, and progress logs.

---

## 17) Dictionary of Acronyms & Terms

* Keep a maintained glossary file or table (e.g., `docs/reference/dictionary.yml`) for company‑specific terminology.
* Surface glossary expansions in answers when ambiguity is detected.

---

## 18) MCP Gateway & Ports

* **Services**: Agents/API on **:8000**, Docs site on **:8111**, Admin tools on **:8222** (ingest, dictionary, chat & escalations review).
* **Gateway**: MCP server (FastMCP over HTTP); fronted by NGINX if needed.
* **Health checks**: MCP health `GET /health`; logs via `docker compose logs`.

---

## 19) Frontend UX Guidelines

* Modern, practical UI with easy theme overrides; indicate agent activity (typing/streaming) during retrieval.
* Use TanStack Query for data fetching; avoid prop drilling.
* Feature‑slice folders under `src/features/*`; Biome for features; ESLint for legacy.

---

## 20) Pipeline Observability

* Emit stepwise progress events for crawl/ingest/eval with timestamps, stage, and metrics.
* Surface via WebSocket/SSE channels and logs; correlate with `request_id`.

---

## 21) Testing Expectations

* Mirror knowledge‑service verification: pagination, filtering, retrieval correctness, progress tracking.
* Maintain a minimal smoke pack (see §13.9) and domain tests for retrieval precision.

---

## 22) Debug MCP Connection

* Health: `curl http://localhost:8051/health`
* Logs: `docker compose logs`

---

## 23) Code Quality Standards (Frontend/Backend)

### Frontend

* **TypeScript** strict; Vitest + RTL tests.
* **Biome** in `/src/features/` (120 cols, double quotes, trailing commas); **ESLint** for legacy code.

### Backend

* **Python 3.12**; **Ruff** for lint; **Mypy** for types; **Pytest** for tests; 120‑char lines allowed.

---

## 24) API Naming Conventions (Placeholder)

* Add `API_NAMING_CONVENTIONS.md` to repo; keep REST routes as `/api/{resource}` with nested scopes, and request/response types like `CreateXRequest`, `XResponse`.
* Hook factories and naming: `resourceKeys.detail(id)`, `useResource`/`useCreateResource`.

---

## 25) Dev Commands (optional, if present in this repo)

### Frontend

* `npm run dev` (3737), `npm run build`, `npm run test`, `npm run biome:*`, `npm run lint*`, `vitest run src/features/...`.
* TypeScript checks: `npx tsc --noEmit` (all) or grep features only.

### Backend

* Using `uv`: `uv sync --group all`, `uv run python -m src.server.main` (8181), `uv run pytest`, `uv run ruff check --fix`, `uv run mypy src/`.
* Docker compose: up/down, profiles, logs: `docker compose --profile backend up -d`.

---

## 26) Knowledge Platform Playbook (high‑level summary)

* Microservice split: **UI**, **Core API**, **MCP Gateway**, **Conversational Agents**.
* Agents call MCP tools via HTTP JSON‑RPC; avoid direct DB writes.
* Realtime UX via Socket.IO + progress channels; settings surface for provider creds and RAG toggles.
* Base agent class handles retries, rate limits, and progress callbacks; prompts are datastore‑driven and hot‑reloadable.

---
